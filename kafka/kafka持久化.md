---
Title: 每秒1亿条数据，保存7天，kafka是如何处理的？
Keywords: kafka,持久化,带宽问题,数据积压
Desciption: kafka，像我一样优秀
Date: 2020-12-24 23:26:00
LastEditTime: 2020-12-24 23:26:00
---

kafka的应用场景

- 实时日志聚合，支持高容量事件流
- 处理大量的数据积压，支持来自离线系统的周期性数据加载
- 处理低延迟分发，分区，分布式，以及实时处理



### 如何处理大量的数据积压？

熟知的mysql是通过B+树来持久化到磁盘的，查找数据需要从磁盘读取并构建B+树内存数据结构。而Kafka则是：使用linux文件系统和pagecache，放弃内存数据结构

- 即使进程维护了内存数据结构，该数据也可能会被复制到操作系统的 pagecache 中，事实上所有内容都被存储了两份。

- 通过自动访问所有空闲内存将可用缓存的容量至少翻倍，并且通过存储紧凑的字节结构而不是独立的对象，有望将缓存容量再翻一番。 这样使得32GB的机器缓存容量可以达到28-30GB,并且不会有额外的 GC 负担
- 保持 内存cache 和 文件系统之间一致性逻辑，不需要做过多的转化。

- 简单的读写文件，读写操作不会相互影响。数据的大小不在会影响性能。可以无限量的使用物理硬盘，价格便宜，容量充足。可以充分解决大量的数据积压难题



### 如何在1秒内接收并持久化这1亿条数据呢？

存储问题解决了，面临的问题：大量的I/O和字节拷贝

I/O操作主要是 客户端与服务端的消息传递，服务端的数据持久化。

将多个消息打包成一组消息块，一次请求发送一组消息，服务端接收，consumer每次获取多组消息块，大量的I/O操作将不在是主要的性能问题，其实主要矛盾是在服务带宽问题上。producer/broker/consumer 三者使用二进制数据传输，producer 和 consumer 之间使用相同的压缩算法也可以极大的节省带宽问题

sendfile 将数据从 pagecache 直接发送到网络，这样避免重新复制数据。



